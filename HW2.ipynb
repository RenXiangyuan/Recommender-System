{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>CSE 258 Fall 2017 - Homework 2<center>\n",
    "- - -\n",
    "\n",
    "## <center>Xiangyuan Ren<center>\n",
    "### <center> Department of Electrical and Computer Engineering <center>\n",
    "### <center>Email: xir010@eng.ucsd.edu <center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier evaluation \n",
    "### Answers Briefly (*Details shown in following pages*)\n",
    "* \n",
    "#### Q1:\n",
    "* Accuracy on the validation set: 0.71842563148737026\n",
    "* Accuracy on the test set: 0.72004559908801824 \n",
    "#### Q2:\n",
    "* Using new features\n",
    "* Accuracy on the validation set: 0.62590748185036305\n",
    "* Accuracy on the test set: 0.61960760784784308\n",
    "#### Q3:\n",
    "* #positives: 10254\n",
    "* #true negatives: 73\n",
    "* #false positives: 6243\n",
    "* #false negatives: 97\n",
    "* #Balanced Error Rate: 0.498906563595\n",
    "#### Q4:\n",
    "* Balanced Error Rate on the train set: 0.440505888296\n",
    "* Balanced Error Rate on the validation set: 0.444763207313\n",
    "* Balanced Error Rate on the test set: 0.433740167554\n",
    "#### Q5:\n",
    "* The best model: **$\\lambda$** == 100\n",
    "* Balanced Error Rate on the train set: 0.440827067227\n",
    "* Balanced Error Rate on the validation set: 0.444881466162\n",
    "* Balanced Error Rate on the test set: 0.432677468298\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy.optimize\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict\n",
    "from math import exp\n",
    "from math import log\n",
    "import string\n",
    "def ParseDataFromFile(f):\n",
    "    for l in open(f):\n",
    "        yield eval(l)\n",
    "data=list(ParseDataFromFile(\"beer_50000.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Qestion 1: \n",
    "###           Split the data into training, validation, and test sets, via 1/3, 1/3, 1/3 splits. After training on the training set, report the accuracy of the classifier on the validation and test sets\n",
    "* First, shuffle the data.\n",
    "* feature(datum): to build the feature vector like θ0 + θ1 × ‘review/taste’ + θ2 × ‘review/appearance’ + θ3 × ‘review/aroma’+θ4 × ‘review/palate’ + θ5 × ‘review/overall\n",
    "* inner(), sigmoid(), f(), frime(), train(), performance(): to use logistic regression to train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.shuffle(data)            # Shuffle Data Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    feat = [1, datum['review/taste'], datum['review/appearance'], datum['review/aroma'], \n",
    "            datum['review/palate'], datum['review/overall']]\n",
    "    return feat\n",
    "#\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['beer/ABV']>=6.5 for d in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner(x,y):\n",
    "    return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + exp(-x))\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "    loglikelihood = 0\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        loglikelihood -= log(1 + exp(-logit))\n",
    "        if not y[i]:\n",
    "            loglikelihood -= logit\n",
    "    for k in range(len(theta)):\n",
    "        loglikelihood -= lam * theta[k]*theta[k]\n",
    "  # for debugging\n",
    "  # print(\"ll =\" + str(loglikelihood))\n",
    "    return -loglikelihood\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "    dl = [0]*len(theta)\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        for k in range(len(theta)):\n",
    "            dl[k] += X[i][k] * (1 - sigmoid(logit))\n",
    "            if not y[i]:\n",
    "                dl[k] -= X[i][k]\n",
    "    for k in range(len(theta)):\n",
    "        dl[k] -= lam*2*theta[k]\n",
    "    return numpy.array([-x for x in dl])\n",
    "\n",
    "def train(lam):\n",
    "    theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, pgtol = 10, args = (X_train, y_train, lam))\n",
    "    return theta\n",
    "def performance(theta):\n",
    "    \n",
    "#     scores = [inner(theta,x) for x in X]\n",
    "#     predictions = [s > 0 for s in scores]\n",
    "#     correct = [(a==b) for (a,b) in zip(predictions,y_train)]\n",
    "#     acc = sum(correct) * 1.0 / len(correct)\n",
    "#     return acc\n",
    "    scores_train = [inner(theta,x) for x in X_train]\n",
    "    scores_validate = [inner(theta,x) for x in X_validate]\n",
    "    scores_test = [inner(theta,x) for x in X_test]\n",
    "\n",
    "    predictions_train = [s > 0 for s in scores_train]\n",
    "    predictions_validate = [s >0 for s in scores_validate]\n",
    "    predictions_test = [s > 0 for s in scores_test]\n",
    "\n",
    "    correct_train = [(a==b) for (a,b) in zip(predictions_train,y_train)]\n",
    "    correct_validate = [(a==b) for (a,b) in zip(predictions_validate,y_validate)]\n",
    "    correct_test = [(a==b) for (a,b) in zip(predictions_test,y_test)]\n",
    "  \n",
    "    acc_train = sum(correct_train) * 1.0 / len(correct_train)\n",
    "    acc_validate = sum(correct_validate) * 1.0 / len(correct_validate)\n",
    "    acc_test = sum(correct_test) * 1.0 / len(correct_test)\n",
    "    return acc_train, acc_validate, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:int(len(X)/3)]\n",
    "y_train = y[:int(len(y)/3)]\n",
    "X_validate = X[int(len(X)/3):int(2*len(X)/3)]\n",
    "y_validate = y[int(len(y)/3):int(2*len(y)/3)]\n",
    "X_test = X[int(2*len(X)/3):]\n",
    "y_test = y[int(2*len(X)/3):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta =  [-6.01898262  0.78253351  0.41889885  0.79923473  0.63163875 -0.93382104]\n",
      "lambda = 1.0:\taccuracy=(0.71786871474858993, 0.71842563148737026, 0.72004559908801824)\n"
     ]
    }
   ],
   "source": [
    "lam = 1.0\n",
    "\n",
    "theta = train(lam)\n",
    "print \"theta = \",theta\n",
    "acc = performance(theta)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy=\" + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer Q1:\n",
    "* ( lambda is: 1.0)\n",
    "* ( theta is: [-6.01898262  0.78253351  0.41889885  0.79923473  0.63163875 -0.93382104] )\n",
    "* ( Accuracy on the training set: 0.71786871474858993 )\n",
    "* Accuracy on the validation set: 0.71842563148737026\n",
    "* Accuracy on the test set:  0.72004559908801824"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Q2:\n",
    "### p(positive label) = σ(θ0 + θ1 × #‘lactic’ + θ2 × #‘tart’...),\n",
    "* the feature function should be changed first\n",
    "\n",
    "```\n",
    "reviewcontent=datum['review/text'].strip().translate(None, string.punctuation).lower().split()\n",
    "\n",
    "```\n",
    "\n",
    "* Then we re-run the code again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    feat=[1]*11\n",
    "    reviewcontent=datum['review/text'].strip().translate(None, string.punctuation).lower().split()\n",
    "    featureword=[\"lactic\",\"tart\",\"sour\",\"citric\",\"sweet\",\"acid\",\"hop\",\"fruit\",\"salt\",\"spicy\"]\n",
    "    for i in range(1,11):\n",
    "        feat[i]=sum([featureword[i-1]==j for j in reviewcontent])\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testlambda(lam):\n",
    "    theta = train(lam)\n",
    "    print \"theta = \",theta\n",
    "    acc = performance(theta)\n",
    "    print(\"lambda = \" + str(lam) + \":\\taccuracy=\" + str(acc))\n",
    "    return theta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta =  [ 0.34042945  0.05371304  0.48523357 -0.27661991 -0.11306548  0.23566173\n",
      "  0.03792651 -0.01502086  0.44948817 -0.08324426 -0.18300371]\n",
      "lambda = 1.0:\taccuracy=(0.62246489859594378, 0.62590748185036305, 0.61960760784784308)\n"
     ]
    }
   ],
   "source": [
    "X = [feature(d) for d in data]\n",
    "y = [d['beer/ABV']>=6.5 for d in data]\n",
    "X_train = X[:int(len(X)/3)]\n",
    "y_train = y[:int(len(y)/3)]\n",
    "X_validate = X[int(len(X)/3):int(2*len(X)/3)]\n",
    "y_validate = y[int(len(y)/3):int(2*len(y)/3)]\n",
    "X_test = X[int(2*len(X)/3):]\n",
    "y_test = y[int(2*len(X)/3):]\n",
    "\n",
    "lam = 1.0\n",
    "\n",
    "theta=testlambda(lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "#### Answer Q2:\n",
    "* ( lambda is: 1.0)\n",
    "* ( theta is: [ 0.34042945  0.05371304  0.48523357 -0.27661991 -0.11306548  0.23566173\n",
    "  0.03792651 -0.01502086  0.44948817 -0.08324426 -0.18300371] )\n",
    "* ( Accuracy on the training set: 0.62246489859594378 )\n",
    "* Accuracy on the validation set: 0.62590748185036305\n",
    "* Accuracy on the training set:  0.61960760784784308\n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q3:\n",
    "#### Report the number of true positives, true negatives, false positives, false negatives, and the Balanced Error Rate of the classifier on the test set\n",
    "* First design the function to test TP/FP/FN/TN/FPR/FNR/BER\n",
    "* Then run testerror() on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testerror(X_test,y_test):\n",
    "    scores_test = [inner(theta,x) for x in X_test]\n",
    "    predictions_test = [s > 0 for s in scores_test]\n",
    "    TP=sum([a and b for (a,b) in zip(predictions_test,y_test)])\n",
    "    FP=sum([a and not b for (a,b) in zip(predictions_test,y_test)])\n",
    "    FN=sum([not a and b for (a,b) in zip(predictions_test,y_test)])\n",
    "    TN=sum([not a and not b for (a,b) in zip(predictions_test,y_test)])\n",
    "    FPR=FP/float(FP+TN)\n",
    "    FNR=FN/float(FN+TP)\n",
    "    BER=(FPR+FNR)/2\n",
    "    print \"TP,FP,FN,TN:\\t\",TP,FP,FN,TN\n",
    "    print \"FPR,FNR:\\t\",FPR,FNR\n",
    "    print \"BER:\\t\\t\",BER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP,FP,FN,TN:\t10254 6243 97 73\n",
      "FPR,FNR:\t0.988442051932 0.00937107525843\n",
      "BER:\t\t0.498906563595\n"
     ]
    }
   ],
   "source": [
    "testerror(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer Q3:\n",
    "* #positives: 10254\n",
    "* #true negatives: 73\n",
    "* #false positives: 6243\n",
    "* #false negatives: 97\n",
    "* #Balanced Error Rate: 0.498906563595\n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q4:\n",
    "#### Adjust the gradient ascent code provided such that the classifier would be approximately ‘balanced’ between the positive and negative classes\n",
    "* First change the log-likelihood based on its label (add weights to each label)\n",
    "```\n",
    "if y[i]:\n",
    "            loglikelihood -=N/float(2*Y1)* log(1 + exp(-logit))\n",
    "        if not y[i]:\n",
    "            loglikelihood -= N/float(2*Y0)*(log(1+exp(-logit))+logit)\n",
    "```\n",
    "* Then change the frime accordingly\n",
    "```\n",
    "if y[i]:\n",
    "                dl[k] +=N/float(2*Y1)* X[i][k] * (1 - sigmoid(logit))\n",
    "            if not y[i]:\n",
    "                dl[k] +=N/float(2*Y0)* X[i][k] * (1 - sigmoid(logit))\n",
    "                dl[k] -=N/float(2*Y0)* X[i][k]\n",
    "```\n",
    "* Report the Balanced Error Rate (on the train/validation/test sets) for the new classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "    N=len(y)\n",
    "    Y1=sum(y)\n",
    "    Y0=N-Y1\n",
    "    loglikelihood = 0\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        if y[i]:\n",
    "            loglikelihood -=N/float(2*Y1)* log(1 + exp(-logit))\n",
    "        if not y[i]:\n",
    "            loglikelihood -= N/float(2*Y0)*(log(1+exp(-logit))+logit)\n",
    "    for k in range(len(theta)):\n",
    "        loglikelihood -= lam * theta[k]*theta[k]\n",
    "  # for debugging\n",
    "  # print(\"ll =\" + str(loglikelihood))\n",
    "    return -loglikelihood\n",
    "\n",
    "def fprime(theta, X, y, lam):\n",
    "    N=len(y)\n",
    "    Y1=sum(y)\n",
    "    Y0=N-Y1\n",
    "    dl = [0]*len(theta)\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        for k in range(len(theta)):\n",
    "            if y[i]:\n",
    "                dl[k] +=N/float(2*Y1)* X[i][k] * (1 - sigmoid(logit))\n",
    "            if not y[i]:\n",
    "                dl[k] +=N/float(2*Y0)* X[i][k] * (1 - sigmoid(logit))\n",
    "                dl[k] -=N/float(2*Y0)* X[i][k]\n",
    "    for k in range(len(theta)):\n",
    "        dl[k] -= lam*2*theta[k]\n",
    "    return numpy.array([-x for x in dl])\n",
    "def train(lam):\n",
    "    theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, pgtol = 10, args = (X_train, y_train, lam))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta =  [-0.15741912  0.06659128  0.52613262 -0.26389162 -0.13014552  0.23019588\n",
      "  0.04661892 -0.01709154  0.43591598 -0.10288322 -0.16390937]\n",
      "lambda = 1.0:\taccuracy=(0.53288131525261007, 0.52732945341093174, 0.53986920261594773)\n"
     ]
    }
   ],
   "source": [
    "lam = 1.0\n",
    "\n",
    "theta=testlambda(lam)\n",
    "# theta = train(lam)\n",
    "# acc = performance(theta)\n",
    "# print(\"lambda = \" + str(lam) + \":\\taccuracy=\" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:\n",
      "TP,FP,FN,TN:\t4675 2088 5697 4206\n",
      "FPR,FNR:\t0.331744518589 0.549267258002\n",
      "BER:\t\t0.440505888296\n"
     ]
    }
   ],
   "source": [
    "print \"TRAIN:\"\n",
    "testerror(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATE:\n",
      "TP,FP,FN,TN:\t4650 2082 5796 4139\n",
      "FPR,FNR:\t0.334672882173 0.554853532453\n",
      "BER:\t\t0.444763207313\n"
     ]
    }
   ],
   "source": [
    "print \"VALIDATE:\"\n",
    "testerror(X_validate,y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST:\n",
      "TP,FP,FN,TN:\t4733 2051 5618 4265\n",
      "FPR,FNR:\t0.324730842305 0.542749492803\n",
      "BER:\t\t0.433740167554\n"
     ]
    }
   ],
   "source": [
    "print \"TEST:\"\n",
    "testerror(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer Q4:\n",
    "* Balanced Error Rate on the train set: 0.440505888296\n",
    "* Balanced Error Rate on the validation set: 0.444763207313\n",
    "* Balanced Error Rate on the test set: 0.433740167554\n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q5:\n",
    "####  Implement a training/validation/test pipeline so that you can select the best model based on its perfor- mance on the validation set.\n",
    "* Run logistic-regressor on all the $\\lambda$\n",
    "* Pick the best $\\lambda$, which has best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta =  [-0.15783973  0.06778283  0.53295583 -0.26693821 -0.13215964  0.23036407\n",
      "  0.04743829 -0.01692134  0.43740362 -0.10470616 -0.16447784]\n",
      "lambda = 0:\taccuracy=(0.53282131285251411, 0.52738945221095579, 0.53986920261594773)\n",
      "\n",
      "theta =  [-0.1578355   0.06777073  0.53288658 -0.26690732 -0.13213921  0.23036239\n",
      "  0.04742998 -0.01692307  0.43738865 -0.10468766 -0.16447212]\n",
      "lambda = 0.01:\taccuracy=(0.53282131285251411, 0.52738945221095579, 0.53986920261594773)\n",
      "\n",
      "theta =  [-0.15779741  0.06766208  0.53226426 -0.2666297  -0.13195561  0.23034727\n",
      "  0.04735527 -0.01693856  0.43725396 -0.10452145 -0.16442071]\n",
      "lambda = 0.1:\taccuracy=(0.53282131285251411, 0.52738945221095579, 0.53986920261594773)\n",
      "\n",
      "theta =  [-0.15741912  0.06659128  0.52613262 -0.26389162 -0.13014552  0.23019588\n",
      "  0.04661892 -0.01709154  0.43591598 -0.10288322 -0.16390937]\n",
      "lambda = 1:\taccuracy=(0.53288131525261007, 0.52732945341093174, 0.53986920261594773)\n",
      "\n",
      "theta =  [-0.12910281  0.02039949  0.24784724 -0.12986642 -0.04861796  0.21283683\n",
      "  0.01472938 -0.02318686  0.34009174 -0.03182648 -0.12248925]\n",
      "lambda = 100:\taccuracy=(0.53318132725309009, 0.52762944741105178, 0.54118917621647566)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lam in [0,0.01,0.1,1,100]:\n",
    "    theta=testlambda(lam)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### When lambda=100, I have best performance on  validation set, which is 0.52762944741105178. So, I pick it as my best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta =  [-0.12910281  0.02039949  0.24784724 -0.12986642 -0.04861796  0.21283683\n",
      "  0.01472938 -0.02318686  0.34009174 -0.03182648 -0.12248925]\n",
      "lambda = 100:\taccuracy=(0.53318132725309009, 0.52762944741105178, 0.54118917621647566)\n",
      "TRAIN:\n",
      "TP,FP,FN,TN:\t4698 2106 5674 4188\n",
      "FPR,FNR:\t0.334604385129 0.547049749325\n",
      "BER:\t\t0.440827067227\n",
      "VALIDATE:\n",
      "TP,FP,FN,TN:\t4666 2093 5780 4128\n",
      "FPR,FNR:\t0.336441086642 0.553321845683\n",
      "BER:\t\t0.444881466162\n",
      "TEST:\n",
      "TP,FP,FN,TN:\t4755 2051 5596 4265\n",
      "FPR,FNR:\t0.324730842305 0.54062409429\n",
      "BER:\t\t0.432677468298\n"
     ]
    }
   ],
   "source": [
    "theta=testlambda(100)\n",
    "print \"TRAIN:\"\n",
    "testerror(X_train,y_train)\n",
    "print \"VALIDATE:\"\n",
    "testerror(X_validate,y_validate)\n",
    "print \"TEST:\"\n",
    "testerror(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5:\n",
    "* The best model: **$\\lambda$** == 100\n",
    "* Balanced Error Rate on the train set: 0.440827067227\n",
    "* Balanced Error Rate on the validation set: 0.444881466162\n",
    "* Balanced Error Rate on the test set: 0.432677468298"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "\n",
    "---\n",
    "## Part II: Dimensionality reduction\n",
    "### Q6:\n",
    "#### Find and report the PCA components \n",
    "* First get the 10-d data:\n",
    "```\n",
    "X10d=[x[1:] for x in X]\n",
    "```\n",
    "* Then report PCA for 1-10components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X10d=[x[1:] for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 0, 0, 0, 3, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X10d[0],X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When n_components= 1 \n",
      "[[  3.84781897e-04   2.93550360e-02   9.41570014e-03   3.70747587e-03\n",
      "    9.76398248e-01  -4.53936670e-05   1.95730560e-01   8.29094610e-02\n",
      "    2.87235246e-05   2.23105359e-02]] \n",
      "\n",
      "When n_components= 2 \n",
      "[[  3.84781897e-04   2.93550360e-02   9.41570014e-03   3.70747587e-03\n",
      "    9.76398248e-01  -4.53936670e-05   1.95730560e-01   8.29094610e-02\n",
      "    2.87235246e-05   2.23105359e-02]\n",
      " [ -8.73841023e-04  -2.12881276e-02  -1.52948443e-02   1.62711131e-02\n",
      "   -1.96510401e-01   3.68488368e-04   9.79233491e-01   5.07531808e-04\n",
      "    1.50338567e-04   3.91457090e-02]] \n",
      "\n",
      "When n_components= 3 \n",
      "[[  3.84781897e-04   2.93550360e-02   9.41570014e-03   3.70747587e-03\n",
      "    9.76398248e-01  -4.53936670e-05   1.95730560e-01   8.29094610e-02\n",
      "    2.87235246e-05   2.23105359e-02]\n",
      " [ -8.73841023e-04  -2.12881276e-02  -1.52948443e-02   1.62711131e-02\n",
      "   -1.96510401e-01   3.68488368e-04   9.79233491e-01   5.07531808e-04\n",
      "    1.50338567e-04   3.91457090e-02]\n",
      " [  1.07750032e-03   1.27386243e-01   7.54590139e-02   2.78797250e-03\n",
      "   -8.54409326e-02   1.01260929e-03  -1.36862786e-02   9.85179240e-01\n",
      "    4.48334945e-04  -1.70882783e-03]] \n",
      "\n",
      "When n_components= 4 \n",
      "[[  3.84781897e-04   2.93550360e-02   9.41570014e-03   3.70747587e-03\n",
      "    9.76398248e-01  -4.53936670e-05   1.95730560e-01   8.29094610e-02\n",
      "    2.87235246e-05   2.23105359e-02]\n",
      " [ -8.73841023e-04  -2.12881276e-02  -1.52948443e-02   1.62711131e-02\n",
      "   -1.96510401e-01   3.68488368e-04   9.79233491e-01   5.07531808e-04\n",
      "    1.50338567e-04   3.91457090e-02]\n",
      " [  1.07750032e-03   1.27386243e-01   7.54590139e-02   2.78797250e-03\n",
      "   -8.54409326e-02   1.01260929e-03  -1.36862786e-02   9.85179240e-01\n",
      "    4.48334945e-04  -1.70882783e-03]\n",
      " [ -4.51179020e-04   7.82751883e-03   7.28227279e-03   1.46779738e-02\n",
      "   -1.44595393e-02  -3.72728740e-04  -4.27895653e-02  -1.72687872e-03\n",
      "    7.30775673e-04   9.98812481e-01]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,4+1):\n",
    "    pca = PCA(n_components=i)\n",
    "    pca.fit(X10d)\n",
    "    print \"When n_components=\",i,'\\n',pca.components_,'\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When n_components= 5 \n",
      "[[  3.84781897e-04   2.93550360e-02   9.41570014e-03   3.70747587e-03\n",
      "    9.76398248e-01  -4.53936670e-05   1.95730560e-01   8.29094610e-02\n",
      "    2.87235246e-05   2.23105359e-02]\n",
      " [ -8.73841023e-04  -2.12881276e-02  -1.52948443e-02   1.62711131e-02\n",
      "   -1.96510401e-01   3.68488368e-04   9.79233491e-01   5.07531808e-04\n",
      "    1.50338567e-04   3.91457090e-02]\n",
      " [  1.07750032e-03   1.27386243e-01   7.54590139e-02   2.78797250e-03\n",
      "   -8.54409326e-02   1.01260929e-03  -1.36862786e-02   9.85179240e-01\n",
      "    4.48334945e-04  -1.70882783e-03]\n",
      " [ -4.51179020e-04   7.82751883e-03   7.28227279e-03   1.46779738e-02\n",
      "   -1.44595393e-02  -3.72728740e-04  -4.27895653e-02  -1.72687872e-03\n",
      "    7.30775673e-04   9.98812481e-01]\n",
      " [  1.39047801e-02   6.11500916e-01   7.77931329e-01   1.13638384e-02\n",
      "   -1.82411489e-02   6.37557442e-03   2.20844753e-02  -1.40001081e-01\n",
      "    1.37120014e-03  -1.01834248e-02]] \n",
      "\n",
      "When n_components= 6 \n",
      "[[  3.84781897e-04   2.93550360e-02   9.41570014e-03   3.70747587e-03\n",
      "    9.76398248e-01  -4.53936670e-05   1.95730560e-01   8.29094610e-02\n",
      "    2.87235246e-05   2.23105359e-02]\n",
      " [ -8.73841023e-04  -2.12881276e-02  -1.52948443e-02   1.62711131e-02\n",
      "   -1.96510401e-01   3.68488368e-04   9.79233491e-01   5.07531808e-04\n",
      "    1.50338567e-04   3.91457090e-02]\n",
      " [  1.07750032e-03   1.27386243e-01   7.54590139e-02   2.78797250e-03\n",
      "   -8.54409326e-02   1.01260929e-03  -1.36862786e-02   9.85179240e-01\n",
      "    4.48334945e-04  -1.70882783e-03]\n",
      " [ -4.51179020e-04   7.82751883e-03   7.28227279e-03   1.46779738e-02\n",
      "   -1.44595393e-02  -3.72728740e-04  -4.27895653e-02  -1.72687872e-03\n",
      "    7.30775673e-04   9.98812481e-01]\n",
      " [  1.39047801e-02   6.11500916e-01   7.77931329e-01   1.13638384e-02\n",
      "   -1.82411489e-02   6.37557442e-03   2.20844753e-02  -1.40001081e-01\n",
      "    1.37120014e-03  -1.01834248e-02]\n",
      " [ -4.03744089e-04  -7.79885511e-01   6.23370361e-01  -5.51135085e-03\n",
      "    1.37075104e-02  -1.16422329e-03  -4.47316463e-03   5.42423779e-02\n",
      "   -1.69982762e-03   1.74908365e-03]] \n",
      "\n",
      "When n_components= 7 \n",
      "[[  3.84781897e-04   2.93550360e-02   9.41570014e-03   3.70747587e-03\n",
      "    9.76398248e-01  -4.53936670e-05   1.95730560e-01   8.29094610e-02\n",
      "    2.87235246e-05   2.23105359e-02]\n",
      " [ -8.73841023e-04  -2.12881276e-02  -1.52948443e-02   1.62711131e-02\n",
      "   -1.96510401e-01   3.68488368e-04   9.79233491e-01   5.07531808e-04\n",
      "    1.50338567e-04   3.91457090e-02]\n",
      " [  1.07750032e-03   1.27386243e-01   7.54590139e-02   2.78797250e-03\n",
      "   -8.54409326e-02   1.01260929e-03  -1.36862786e-02   9.85179240e-01\n",
      "    4.48334945e-04  -1.70882783e-03]\n",
      " [ -4.51179020e-04   7.82751883e-03   7.28227279e-03   1.46779738e-02\n",
      "   -1.44595393e-02  -3.72728740e-04  -4.27895653e-02  -1.72687872e-03\n",
      "    7.30775673e-04   9.98812481e-01]\n",
      " [  1.39047801e-02   6.11500916e-01   7.77931329e-01   1.13638384e-02\n",
      "   -1.82411489e-02   6.37557442e-03   2.20844753e-02  -1.40001081e-01\n",
      "    1.37120014e-03  -1.01834248e-02]\n",
      " [ -4.03744089e-04  -7.79885511e-01   6.23370361e-01  -5.51135085e-03\n",
      "    1.37075104e-02  -1.16422329e-03  -4.47316463e-03   5.42423779e-02\n",
      "   -1.69982762e-03   1.74908365e-03]\n",
      " [  4.02303337e-03  -1.15707251e-02  -5.59667233e-03   9.99611141e-01\n",
      "    3.13336301e-04   9.84579602e-03  -1.62758636e-02  -1.14421414e-03\n",
      "    6.53824555e-04  -1.52479220e-02]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5,7+1):\n",
    "    pca = PCA(n_components=i)\n",
    "    pca.fit(X10d)\n",
    "    print \"When n_components=\",i,'\\n',pca.components_,'\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When n_components= 8 \n",
      "[[  3.84781897e-04   2.93550360e-02   9.41570014e-03   3.70747587e-03\n",
      "    9.76398248e-01  -4.53936670e-05   1.95730560e-01   8.29094610e-02\n",
      "    2.87235246e-05   2.23105359e-02]\n",
      " [ -8.73841023e-04  -2.12881276e-02  -1.52948443e-02   1.62711131e-02\n",
      "   -1.96510401e-01   3.68488368e-04   9.79233491e-01   5.07531808e-04\n",
      "    1.50338567e-04   3.91457090e-02]\n",
      " [  1.07750032e-03   1.27386243e-01   7.54590139e-02   2.78797250e-03\n",
      "   -8.54409326e-02   1.01260929e-03  -1.36862786e-02   9.85179240e-01\n",
      "    4.48334945e-04  -1.70882783e-03]\n",
      " [ -4.51179020e-04   7.82751883e-03   7.28227279e-03   1.46779738e-02\n",
      "   -1.44595393e-02  -3.72728740e-04  -4.27895653e-02  -1.72687872e-03\n",
      "    7.30775673e-04   9.98812481e-01]\n",
      " [  1.39047801e-02   6.11500916e-01   7.77931329e-01   1.13638384e-02\n",
      "   -1.82411489e-02   6.37557442e-03   2.20844753e-02  -1.40001081e-01\n",
      "    1.37120014e-03  -1.01834248e-02]\n",
      " [ -4.03744089e-04  -7.79885511e-01   6.23370361e-01  -5.51135085e-03\n",
      "    1.37075104e-02  -1.16422329e-03  -4.47316463e-03   5.42423779e-02\n",
      "   -1.69982762e-03   1.74908365e-03]\n",
      " [  4.02303337e-03  -1.15707251e-02  -5.59667233e-03   9.99611141e-01\n",
      "    3.13336301e-04   9.84579602e-03  -1.62758636e-02  -1.14421414e-03\n",
      "    6.53824555e-04  -1.52479220e-02]\n",
      " [  9.94092033e-01  -9.38302334e-03  -1.10326770e-02  -5.20118822e-03\n",
      "   -1.68472241e-04   1.07160210e-01   4.94021992e-04   8.72664903e-04\n",
      "   -7.70175338e-03   7.45310085e-04]] \n",
      "\n",
      "When n_components= 9 \n",
      "[[  3.84781897e-04   2.93550360e-02   9.41570014e-03   3.70747587e-03\n",
      "    9.76398248e-01  -4.53936670e-05   1.95730560e-01   8.29094610e-02\n",
      "    2.87235246e-05   2.23105359e-02]\n",
      " [ -8.73841023e-04  -2.12881276e-02  -1.52948443e-02   1.62711131e-02\n",
      "   -1.96510401e-01   3.68488368e-04   9.79233491e-01   5.07531808e-04\n",
      "    1.50338567e-04   3.91457090e-02]\n",
      " [  1.07750032e-03   1.27386243e-01   7.54590139e-02   2.78797250e-03\n",
      "   -8.54409326e-02   1.01260929e-03  -1.36862786e-02   9.85179240e-01\n",
      "    4.48334945e-04  -1.70882783e-03]\n",
      " [ -4.51179020e-04   7.82751883e-03   7.28227279e-03   1.46779738e-02\n",
      "   -1.44595393e-02  -3.72728740e-04  -4.27895653e-02  -1.72687872e-03\n",
      "    7.30775673e-04   9.98812481e-01]\n",
      " [  1.39047801e-02   6.11500916e-01   7.77931329e-01   1.13638384e-02\n",
      "   -1.82411489e-02   6.37557442e-03   2.20844753e-02  -1.40001081e-01\n",
      "    1.37120014e-03  -1.01834248e-02]\n",
      " [ -4.03744089e-04  -7.79885511e-01   6.23370361e-01  -5.51135085e-03\n",
      "    1.37075104e-02  -1.16422329e-03  -4.47316463e-03   5.42423779e-02\n",
      "   -1.69982762e-03   1.74908365e-03]\n",
      " [  4.02303337e-03  -1.15707251e-02  -5.59667233e-03   9.99611141e-01\n",
      "    3.13336301e-04   9.84579602e-03  -1.62758636e-02  -1.14421414e-03\n",
      "    6.53824555e-04  -1.52479220e-02]\n",
      " [  9.94092033e-01  -9.38302334e-03  -1.10326770e-02  -5.20118822e-03\n",
      "   -1.68472241e-04   1.07160210e-01   4.94021992e-04   8.72664903e-04\n",
      "   -7.70175338e-03   7.45310085e-04]\n",
      " [  8.82766450e-03  -2.24670412e-03  -9.12067230e-05  -6.28236833e-04\n",
      "    9.33155012e-05  -1.02566013e-02  -1.34473067e-04  -1.49885078e-04\n",
      "    9.99905432e-01  -7.08580114e-04]] \n",
      "\n",
      "When n_components= 10 \n",
      "[[  3.84781897e-04   2.93550360e-02   9.41570014e-03   3.70747587e-03\n",
      "    9.76398248e-01  -4.53936670e-05   1.95730560e-01   8.29094610e-02\n",
      "    2.87235246e-05   2.23105359e-02]\n",
      " [ -8.73841023e-04  -2.12881276e-02  -1.52948443e-02   1.62711131e-02\n",
      "   -1.96510401e-01   3.68488368e-04   9.79233491e-01   5.07531808e-04\n",
      "    1.50338567e-04   3.91457090e-02]\n",
      " [  1.07750032e-03   1.27386243e-01   7.54590139e-02   2.78797250e-03\n",
      "   -8.54409326e-02   1.01260929e-03  -1.36862786e-02   9.85179240e-01\n",
      "    4.48334945e-04  -1.70882783e-03]\n",
      " [ -4.51179020e-04   7.82751883e-03   7.28227279e-03   1.46779738e-02\n",
      "   -1.44595393e-02  -3.72728740e-04  -4.27895653e-02  -1.72687872e-03\n",
      "    7.30775673e-04   9.98812481e-01]\n",
      " [  1.39047801e-02   6.11500916e-01   7.77931329e-01   1.13638384e-02\n",
      "   -1.82411489e-02   6.37557442e-03   2.20844753e-02  -1.40001081e-01\n",
      "    1.37120014e-03  -1.01834248e-02]\n",
      " [ -4.03744089e-04  -7.79885511e-01   6.23370361e-01  -5.51135085e-03\n",
      "    1.37075104e-02  -1.16422329e-03  -4.47316463e-03   5.42423779e-02\n",
      "   -1.69982762e-03   1.74908365e-03]\n",
      " [  4.02303337e-03  -1.15707251e-02  -5.59667233e-03   9.99611141e-01\n",
      "    3.13336301e-04   9.84579602e-03  -1.62758636e-02  -1.14421414e-03\n",
      "    6.53824555e-04  -1.52479220e-02]\n",
      " [  9.94092033e-01  -9.38302334e-03  -1.10326770e-02  -5.20118822e-03\n",
      "   -1.68472241e-04   1.07160210e-01   4.94021992e-04   8.72664903e-04\n",
      "   -7.70175338e-03   7.45310085e-04]\n",
      " [  8.82766450e-03  -2.24670412e-03  -9.12067230e-05  -6.28236833e-04\n",
      "    9.33155012e-05  -1.02566013e-02  -1.34473067e-04  -1.49885078e-04\n",
      "    9.99905432e-01  -7.08580114e-04]\n",
      " [ -1.07196716e-01  -3.84980557e-03  -3.08335266e-03  -9.42855330e-03\n",
      "    3.48092236e-04   9.94118326e-01  -3.96451954e-04  -1.23443833e-04\n",
      "    1.11290158e-02   4.93461116e-04]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(8,10+1):\n",
    "    pca = PCA(n_components=i)\n",
    "    pca.fit(X10d)\n",
    "    print \"When n_components=\",i,'\\n',pca.components_,'\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "---\n",
    "### Q7:\n",
    "#### Suppose we want to compress the data using just two PCA dimensions. How large is the reconstruction error when doing so\n",
    "* Get the 2-d data and the reconstruction:\n",
    "```\n",
    "X_train_pca = pca.transform(X10d)\n",
    "X_projected = pca.inverse_transform(X_train_pca)\n",
    "```\n",
    "* Calculate the reconstruction error\n",
    "```\n",
    "loss = ((X10d - X_projected) ** 2).mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.56068615804e-31\n"
     ]
    }
   ],
   "source": [
    "X_train_pca = pca.transform(X10d)\n",
    "X_projected = pca.inverse_transform(X_train_pca)\n",
    "loss = ((X10d - X_projected) ** 2).mean()\n",
    "print loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer Q7:\n",
    "* the reconstruction error is: 1.56068615804e-31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "\n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "\n",
    "---\n",
    "### Q8:\n",
    "#### make a 2-d scatterplot showing the difference between ‘American IPA’ style beers versus all other styles* Get the 2-d data and the reconstruction:\n",
    "* First split the 2-d into X and Y:\n",
    "```\n",
    "x =X_train_pca[:,0]  \n",
    "y =X_train_pca[:,1] \n",
    "```\n",
    "* Then get the color according to its label\n",
    "```\n",
    "cValue = []\n",
    "for i in C:\n",
    "    if i==1:\n",
    "        cValue.append('r')\n",
    "    else:\n",
    "        cValue.append('b')\n",
    "```\n",
    "* Using **matplotlib.pyplot** to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEWCAYAAACNJFuYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxOcX0pYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b30ecd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x =X_train_pca[:,0]  \n",
    "y =X_train_pca[:,1] \n",
    "C=[0]*50000\n",
    "for i in range(50000):\n",
    "    if data[i]['beer/style']== 'American IPA':\n",
    "        C[i]=1\n",
    "fig = plt.figure()  \n",
    "ax1 = fig.add_subplot(111)  \n",
    "\n",
    "ax1.set_title('Plotting American IPA v.s. others')  \n",
    "plt.xlabel('X')  \n",
    "plt.ylabel('Y')  \n",
    "\n",
    "cValue = []\n",
    "for i in C:\n",
    "    if i==1:\n",
    "        cValue.append('r')\n",
    "    else:\n",
    "        cValue.append('b')\n",
    "\n",
    "ax1.scatter(x,y,c=cValue,marker='.')   \n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
