{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>CSE 258 Fall 2017 - Homework 2<center>\n",
    "- - -\n",
    "\n",
    "## <center>Xiangyuan Ren<center>\n",
    "### <center> Department of Electrical and Computer Engineering <center>\n",
    "### <center>Email: xir010@eng.ucsd.edu <center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier evaluation \n",
    "### Answers Briefly with shuffle \n",
    "### (*Answers without shuffle are shown in following pages*)\n",
    "* \n",
    "#### Q1:\n",
    "* Accuracy on the validation set: 0.71842563148737026\n",
    "* Accuracy on the test set: 0.72004559908801824 \n",
    "#### Q2:\n",
    "* Using new features\n",
    "* Accuracy on the validation set: 0.62590748185036305\n",
    "* Accuracy on the test set: 0.61960760784784308\n",
    "#### Q3:\n",
    "* #positives: 10254\n",
    "* #true negatives: 73\n",
    "* #false positives: 6243\n",
    "* #false negatives: 97\n",
    "* #Balanced Error Rate: 0.498906563595\n",
    "#### Q4:\n",
    "* Balanced Error Rate on the train set: 0.440505888296\n",
    "* Balanced Error Rate on the validation set: 0.444763207313\n",
    "* Balanced Error Rate on the test set: 0.433740167554\n",
    "#### Q5:\n",
    "* The best model: **$\\lambda$** == 100\n",
    "* Balanced Error Rate on the train set: 0.440827067227\n",
    "* Balanced Error Rate on the validation set: 0.444881466162\n",
    "* Balanced Error Rate on the test set: 0.432677468298\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy.optimize\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict\n",
    "from math import exp\n",
    "from math import log\n",
    "import string\n",
    "def ParseDataFromFile(f):\n",
    "    for l in open(f):\n",
    "        yield eval(l)\n",
    "data=list(ParseDataFromFile(\"beer_50000.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Qestion 1: \n",
    "###           Split the data into training, validation, and test sets, via 1/3, 1/3, 1/3 splits. After training on the training set, report the accuracy of the classifier on the validation and test sets\n",
    "* No shuffle data.\n",
    "* feature(datum): to build the feature vector like θ0 + θ1 × ‘review/taste’ + θ2 × ‘review/appearance’ + θ3 × ‘review/aroma’+θ4 × ‘review/palate’ + θ5 × ‘review/overall\n",
    "* inner(), sigmoid(), f(), frime(), train(), performance(): to use logistic regression to train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy.random.shuffle(data)            # Shuffle Data Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    feat = [1, datum['review/taste'], datum['review/appearance'], datum['review/aroma'], \n",
    "            datum['review/palate'], datum['review/overall']]\n",
    "    return feat\n",
    "#\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['beer/ABV']>=6.5 for d in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner(x,y):\n",
    "    return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + exp(-x))\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "    loglikelihood = 0\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        loglikelihood -= log(1 + exp(-logit))\n",
    "        if not y[i]:\n",
    "            loglikelihood -= logit\n",
    "    for k in range(len(theta)):\n",
    "        loglikelihood -= lam * theta[k]*theta[k]\n",
    "  # for debugging\n",
    "  # print(\"ll =\" + str(loglikelihood))\n",
    "    return -loglikelihood\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "    dl = [0]*len(theta)\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        for k in range(len(theta)):\n",
    "            dl[k] += X[i][k] * (1 - sigmoid(logit))\n",
    "            if not y[i]:\n",
    "                dl[k] -= X[i][k]\n",
    "    for k in range(len(theta)):\n",
    "        dl[k] -= lam*2*theta[k]\n",
    "    return numpy.array([-x for x in dl])\n",
    "\n",
    "def train(lam):\n",
    "    theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, pgtol = 10, args = (X_train, y_train, lam))\n",
    "    return theta\n",
    "def performance(theta):\n",
    "    \n",
    "#     scores = [inner(theta,x) for x in X]\n",
    "#     predictions = [s > 0 for s in scores]\n",
    "#     correct = [(a==b) for (a,b) in zip(predictions,y_train)]\n",
    "#     acc = sum(correct) * 1.0 / len(correct)\n",
    "#     return acc\n",
    "    scores_train = [inner(theta,x) for x in X_train]\n",
    "    scores_validate = [inner(theta,x) for x in X_validate]\n",
    "    scores_test = [inner(theta,x) for x in X_test]\n",
    "\n",
    "    predictions_train = [s > 0 for s in scores_train]\n",
    "    predictions_validate = [s >0 for s in scores_validate]\n",
    "    predictions_test = [s > 0 for s in scores_test]\n",
    "\n",
    "    correct_train = [(a==b) for (a,b) in zip(predictions_train,y_train)]\n",
    "    correct_validate = [(a==b) for (a,b) in zip(predictions_validate,y_validate)]\n",
    "    correct_test = [(a==b) for (a,b) in zip(predictions_test,y_test)]\n",
    "  \n",
    "    acc_train = sum(correct_train) * 1.0 / len(correct_train)\n",
    "    acc_validate = sum(correct_validate) * 1.0 / len(correct_validate)\n",
    "    acc_test = sum(correct_test) * 1.0 / len(correct_test)\n",
    "    return acc_train, acc_validate, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:int(len(X)/3)]\n",
    "y_train = y[:int(len(y)/3)]\n",
    "X_validate = X[int(len(X)/3):int(2*len(X)/3)]\n",
    "y_validate = y[int(len(y)/3):int(2*len(y)/3)]\n",
    "X_test = X[int(2*len(X)/3):]\n",
    "y_test = y[int(2*len(X)/3):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta =  [-4.40507927  0.59370219  0.34404315  0.81246286  0.57522903 -1.06525496]\n",
      "lambda = 1.0:\taccuracy=(0.66986679467178689, 0.9002819943601128, 0.57778844423111542)\n"
     ]
    }
   ],
   "source": [
    "lam = 1.0\n",
    "\n",
    "theta = train(lam)\n",
    "print \"theta = \",theta\n",
    "acc = performance(theta)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy=\" + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer Q1:\n",
    "* ( lambda is: 1.0)\n",
    "* ( theta is: [-4.40507927  0.59370219  0.34404315  0.81246286  0.57522903 -1.06525496] )\n",
    "* ( Accuracy on the training set: 0.66986679467178689 )\n",
    "* Accuracy on the validation set: 0.9002819943601128\n",
    "* Accuracy on the test set:  0.57778844423111542"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Q2:\n",
    "### p(positive label) = σ(θ0 + θ1 × #‘lactic’ + θ2 × #‘tart’...),\n",
    "* the feature function should be changed first\n",
    "\n",
    "```\n",
    "reviewcontent=datum['review/text'].translate(None, string.punctuation).lower().strip().split()\n",
    "\n",
    "```\n",
    "\n",
    "* Then we re-run the code again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    feat=[1]*11\n",
    "    reviewcontent=datum['review/text'].translate(None, string.punctuation).lower()\n",
    "    reviewcontent=reviewcontent.strip().split()\n",
    "    featureword=[\"lactic\",\"tart\",\"sour\",\"citric\",\"sweet\",\"acid\",\"hop\",\"fruit\",\"salt\",\"spicy\"]\n",
    "    for i in range(1,11):\n",
    "        feat[i]=sum([featureword[i-1]==j for j in reviewcontent])\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testlambda(lam):\n",
    "    theta = train(lam)\n",
    "    print \"theta = \",theta\n",
    "    acc = performance(theta)\n",
    "    print(\"lambda = \" + str(lam) + \":\\taccuracy=\" + str(acc))\n",
    "    return theta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta =  [ 0.0572843   0.01233175  0.04761345 -0.0954871  -0.03561509  0.28703378\n",
      "  0.02399778 -0.01049852  0.40460598 -0.00519373  0.0029034 ]\n",
      "lambda = 1.0:\taccuracy=(0.56258250330013204, 0.94630107397852048, 0.36233275334493309)\n"
     ]
    }
   ],
   "source": [
    "X = [feature(d) for d in data]\n",
    "y = [d['beer/ABV']>=6.5 for d in data]\n",
    "X_train = X[:int(len(X)/3)]\n",
    "y_train = y[:int(len(y)/3)]\n",
    "X_validate = X[int(len(X)/3):int(2*len(X)/3)]\n",
    "y_validate = y[int(len(y)/3):int(2*len(y)/3)]\n",
    "X_test = X[int(2*len(X)/3):]\n",
    "y_test = y[int(2*len(X)/3):]\n",
    "\n",
    "lam = 1.0\n",
    "\n",
    "theta=testlambda(lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "#### Answer Q2:\n",
    "* ( lambda is: 1.0)\n",
    "* ( theta is: [ 0.0572843   0.01233175  0.04761345 -0.0954871  -0.03561509  0.28703378\n",
    "  0.02399778 -0.01049852  0.40460598 -0.00519373  0.0029034 ] )\n",
    "* ( Accuracy on the training set: 0.56258250330013204 )\n",
    "* Accuracy on the validation set: 0.94630107397852048\n",
    "* Accuracy on the training set:  0.36233275334493309\n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q3:\n",
    "#### Report the number of true positives, true negatives, false positives, false negatives, and the Balanced Error Rate of the classifier on the test set\n",
    "* First design the function to test TP/FP/FN/TN/FPR/FNR/BER\n",
    "* Then run testerror() on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testerror(X_test,y_test):\n",
    "    scores_test = [inner(theta,x) for x in X_test]\n",
    "    predictions_test = [s > 0 for s in scores_test]\n",
    "    TP=sum([a and b for (a,b) in zip(predictions_test,y_test)])\n",
    "    FP=sum([a and not b for (a,b) in zip(predictions_test,y_test)])\n",
    "    FN=sum([not a and b for (a,b) in zip(predictions_test,y_test)])\n",
    "    TN=sum([not a and not b for (a,b) in zip(predictions_test,y_test)])\n",
    "    FPR=FP/float(FP+TN)\n",
    "    FNR=FN/float(FN+TP)\n",
    "    BER=(FPR+FNR)/2\n",
    "    print \"TP,FP,FN,TN:\\t\",TP,FP,FN,TN\n",
    "    print \"FPR,FNR:\\t\",FPR,FNR\n",
    "    print \"BER:\\t\\t\",BER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP,FP,FN,TN:\t5833 10549 79 206\n",
      "FPR,FNR:\t0.980846118085 0.0133626522327\n",
      "BER:\t\t0.497104385159\n"
     ]
    }
   ],
   "source": [
    "testerror(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer Q3:\n",
    "* #true positives: 5833\n",
    "* #true negatives: 206\n",
    "* #false positives: 10549\n",
    "* #false negatives: 79\n",
    "* #Balanced Error Rate: 0.497104385159\n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q4:\n",
    "#### Adjust the gradient ascent code provided such that the classifier would be approximately ‘balanced’ between the positive and negative classes\n",
    "* First change the log-likelihood based on its label (add weights to each label)\n",
    "```\n",
    "if y[i]:\n",
    "            loglikelihood -=N/float(2*Y1)* log(1 + exp(-logit))\n",
    "        if not y[i]:\n",
    "            loglikelihood -= N/float(2*Y0)*(log(1+exp(-logit))+logit)\n",
    "```\n",
    "* Then change the frime accordingly\n",
    "```\n",
    "if y[i]:\n",
    "                dl[k] +=N/float(2*Y1)* X[i][k] * (1 - sigmoid(logit))\n",
    "            if not y[i]:\n",
    "                dl[k] +=N/float(2*Y0)* X[i][k] * (1 - sigmoid(logit))\n",
    "                dl[k] -=N/float(2*Y0)* X[i][k]\n",
    "```\n",
    "* Report the Balanced Error Rate (on the train/validation/test sets) for the new classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "    N=len(y)\n",
    "    Y1=sum(y)\n",
    "    Y0=N-Y1\n",
    "    loglikelihood = 0\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        if y[i]:\n",
    "            loglikelihood -=N/float(2*Y1)* log(1 + exp(-logit))\n",
    "        if not y[i]:\n",
    "            loglikelihood -= N/float(2*Y0)*(log(1+exp(-logit))+logit)\n",
    "    for k in range(len(theta)):\n",
    "        loglikelihood -= lam * theta[k]*theta[k]\n",
    "  # for debugging\n",
    "  # print(\"ll =\" + str(loglikelihood))\n",
    "    return -loglikelihood\n",
    "\n",
    "def fprime(theta, X, y, lam):\n",
    "    N=len(y)\n",
    "    Y1=sum(y)\n",
    "    Y0=N-Y1\n",
    "    dl = [0]*len(theta)\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        for k in range(len(theta)):\n",
    "            if y[i]:\n",
    "                dl[k] +=N/float(2*Y1)* X[i][k] * (1 - sigmoid(logit))\n",
    "            if not y[i]:\n",
    "                dl[k] +=N/float(2*Y0)* X[i][k] * (1 - sigmoid(logit))\n",
    "                dl[k] -=N/float(2*Y0)* X[i][k]\n",
    "    for k in range(len(theta)):\n",
    "        dl[k] -= lam*2*theta[k]\n",
    "    return numpy.array([-x for x in dl])\n",
    "def train(lam):\n",
    "    theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, pgtol = 10, args = (X_train, y_train, lam))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta =  [-0.18369822  0.0100884   0.04149834 -0.08900256 -0.03354634  0.28539368\n",
      "  0.02061901 -0.0121883   0.397628   -0.00474375  0.00267414]\n",
      "lambda = 1.0:\taccuracy=(0.55412216488659549, 0.47117057658846823, 0.5824683506329873)\n"
     ]
    }
   ],
   "source": [
    "lam = 1.0\n",
    "\n",
    "theta=testlambda(lam)\n",
    "# theta = train(lam)\n",
    "# acc = performance(theta)\n",
    "# print(\"lambda = \" + str(lam) + \":\\taccuracy=\" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:\n",
      "TP,FP,FN,TN:\t4399 2482 4949 4836\n",
      "FPR,FNR:\t0.339163705931 0.529418057338\n",
      "BER:\t\t0.434290881635\n"
     ]
    }
   ],
   "source": [
    "print \"TRAIN:\"\n",
    "testerror(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATE:\n",
      "TP,FP,FN,TN:\t7308 213 8601 545\n",
      "FPR,FNR:\t0.281002638522 0.540637375071\n",
      "BER:\t\t0.410820006797\n"
     ]
    }
   ],
   "source": [
    "print \"VALIDATE:\"\n",
    "testerror(X_validate,y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST:\n",
      "TP,FP,FN,TN:\t2765 3812 3147 6943\n",
      "FPR,FNR:\t0.354439795444 0.532307171854\n",
      "BER:\t\t0.443373483649\n"
     ]
    }
   ],
   "source": [
    "print \"TEST:\"\n",
    "testerror(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer Q4:\n",
    "* Balanced Error Rate on the train set: 0.434290881635\n",
    "* Balanced Error Rate on the validation set: 0.410820006797\n",
    "* Balanced Error Rate on the test set: 0.443373483649\n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q5:\n",
    "####  Implement a training/validation/test pipeline so that you can select the best model based on its perfor- mance on the validation set.\n",
    "* Run logistic-regressor on all the $\\lambda$\n",
    "* Pick the best $\\lambda$, which has best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta =  [-0.18406845  0.0101521   0.04172871 -0.08943733 -0.03373912  0.28564164\n",
      "  0.02075214 -0.0121393   0.39903665 -0.00477337  0.00269198]\n",
      "lambda = 0:\taccuracy=(0.55406216248649942, 0.47063058738825225, 0.58258834823303529)\n",
      "\n",
      "theta =  [-0.18406474  0.01015146  0.0417264  -0.08943296 -0.03373718  0.28563915\n",
      "  0.0207508  -0.0121398   0.3990225  -0.00477308  0.0026918 ]\n",
      "lambda = 0.01:\taccuracy=(0.55412216488659549, 0.47117057658846823, 0.5824683506329873)\n",
      "\n",
      "theta =  [-0.18403135  0.0101457   0.04170556 -0.08939365 -0.03371974  0.28561682\n",
      "  0.02073876 -0.01214423  0.39889526 -0.0047704   0.00269019]\n",
      "lambda = 0.1:\taccuracy=(0.55412216488659549, 0.47117057658846823, 0.5824683506329873)\n",
      "\n",
      "theta =  [-0.18369822  0.0100884   0.04149834 -0.08900256 -0.03354634  0.28539368\n",
      "  0.02061901 -0.0121883   0.397628   -0.00474375  0.00267414]\n",
      "lambda = 1:\taccuracy=(0.55412216488659549, 0.47117057658846823, 0.5824683506329873)\n",
      "\n",
      "theta =  [-0.15422583  0.00577634  0.025877   -0.05942604 -0.0204902   0.26214925\n",
      "  0.011632   -0.01279203  0.29805519 -0.00273439  0.00153136]\n",
      "lambda = 100:\taccuracy=(0.55418216728669145, 0.47159056818863621, 0.58222835543289131)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lam in [0,0.01,0.1,1,100]:\n",
    "    theta=testlambda(lam)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### When lambda=100, I have best performance on  validation set, which is 0.47159056818863621. So, I pick it as my best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta =  [-0.15422583  0.00577634  0.025877   -0.05942604 -0.0204902   0.26214925\n",
      "  0.011632   -0.01279203  0.29805519 -0.00273439  0.00153136]\n",
      "lambda = 100:\taccuracy=(0.55418216728669145, 0.47159056818863621, 0.58222835543289131)\n",
      "TRAIN:\n",
      "TP,FP,FN,TN:\t4404 2486 4944 4832\n",
      "FPR,FNR:\t0.339710303362 0.528883183569\n",
      "BER:\t\t0.434296743465\n",
      "VALIDATE:\n",
      "TP,FP,FN,TN:\t7315 213 8594 545\n",
      "FPR,FNR:\t0.281002638522 0.540197372556\n",
      "BER:\t\t0.410600005539\n",
      "TEST:\n",
      "TP,FP,FN,TN:\t2768 3819 3144 6936\n",
      "FPR,FNR:\t0.355090655509 0.531799729364\n",
      "BER:\t\t0.443445192437\n"
     ]
    }
   ],
   "source": [
    "theta=testlambda(100)\n",
    "print \"TRAIN:\"\n",
    "testerror(X_train,y_train)\n",
    "print \"VALIDATE:\"\n",
    "testerror(X_validate,y_validate)\n",
    "print \"TEST:\"\n",
    "testerror(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5:\n",
    "* The best model: **$\\lambda$** = 100\n",
    "* Balanced Error Rate on the train set: 0.434296743465\n",
    "* Balanced Error Rate on the validation set: 0.410600005539\n",
    "* Balanced Error Rate on the test set: 0.443445192437"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "\n",
    "---\n",
    "## Part II: Dimensionality reduction\n",
    "### Q6:\n",
    "#### Find and report the PCA components \n",
    "* First get the 10-d data:\n",
    "```\n",
    "X10d=[x[1:] for x in X]\n",
    "```\n",
    "* Then report PCA for 1-10components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:int(len(X)/3)]\n",
    "X10d=[x[1:] for x in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When n_components= 1 \n",
      "[[ -5.99348611e-04   3.95180460e-03  -9.30313126e-03   9.77013461e-03\n",
      "    7.99743141e-01  -1.15982082e-04   5.94935746e-01   7.26421070e-02\n",
      "    1.73560182e-04   3.14239951e-02]] \n",
      "\n",
      "When n_components= 2 \n",
      "[[ -5.99348611e-04   3.95180460e-03  -9.30313126e-03   9.77013461e-03\n",
      "    7.99743141e-01  -1.15982082e-04   5.94935746e-01   7.26421070e-02\n",
      "    1.73560182e-04   3.14239951e-02]\n",
      " [ -1.57703251e-03  -8.65486079e-03  -1.41729712e-02   1.36618507e-02\n",
      "   -5.96957130e-01   2.48551765e-04   8.01904848e-01  -3.25375266e-03\n",
      "   -1.24914732e-03   1.06619909e-02]] \n",
      "\n",
      "When n_components= 3 \n",
      "[[ -5.99348611e-04   3.95180460e-03  -9.30313126e-03   9.77013461e-03\n",
      "    7.99743141e-01  -1.15982082e-04   5.94935746e-01   7.26421070e-02\n",
      "    1.73560182e-04   3.14239951e-02]\n",
      " [ -1.57703251e-03  -8.65486079e-03  -1.41729712e-02   1.36618507e-02\n",
      "   -5.96957130e-01   2.48551765e-04   8.01904848e-01  -3.25375266e-03\n",
      "   -1.24914732e-03   1.06619909e-02]\n",
      " [  4.00311919e-03   4.43650130e-02   9.06341269e-02   3.63555020e-03\n",
      "   -6.10090802e-02  -2.42559237e-04  -3.98261666e-02   9.91605188e-01\n",
      "    3.81500215e-04   3.46218809e-02]] \n",
      "\n",
      "When n_components= 4 \n",
      "[[ -5.99348611e-04   3.95180460e-03  -9.30313126e-03   9.77013461e-03\n",
      "    7.99743141e-01  -1.15982082e-04   5.94935746e-01   7.26421070e-02\n",
      "    1.73560182e-04   3.14239951e-02]\n",
      " [ -1.57703251e-03  -8.65486079e-03  -1.41729712e-02   1.36618507e-02\n",
      "   -5.96957130e-01   2.48551765e-04   8.01904848e-01  -3.25375266e-03\n",
      "   -1.24914732e-03   1.06619909e-02]\n",
      " [  4.00311919e-03   4.43650130e-02   9.06341269e-02   3.63555020e-03\n",
      "   -6.10090802e-02  -2.42559237e-04  -3.98261666e-02   9.91605188e-01\n",
      "    3.81500215e-04   3.46218809e-02]\n",
      " [ -4.26440126e-04   2.28634173e-02  -1.25300143e-02   1.95012054e-02\n",
      "   -1.68370978e-02  -1.59706556e-04  -2.62603663e-02  -3.68946765e-02\n",
      "    2.69012381e-03   9.98297656e-01]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,4+1):\n",
    "    pca = PCA(n_components=i)\n",
    "    pca.fit(X10d)\n",
    "    print \"When n_components=\",i,'\\n',pca.components_,'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When n_components= 5 \n",
      "[[ -5.99348611e-04   3.95180460e-03  -9.30313126e-03   9.77013461e-03\n",
      "    7.99743141e-01  -1.15982082e-04   5.94935746e-01   7.26421070e-02\n",
      "    1.73560182e-04   3.14239951e-02]\n",
      " [ -1.57703251e-03  -8.65486079e-03  -1.41729712e-02   1.36618507e-02\n",
      "   -5.96957130e-01   2.48551765e-04   8.01904848e-01  -3.25375266e-03\n",
      "   -1.24914732e-03   1.06619909e-02]\n",
      " [  4.00311919e-03   4.43650130e-02   9.06341269e-02   3.63555020e-03\n",
      "   -6.10090802e-02  -2.42559237e-04  -3.98261666e-02   9.91605188e-01\n",
      "    3.81500215e-04   3.46218809e-02]\n",
      " [ -4.26440126e-04   2.28634173e-02  -1.25300143e-02   1.95012054e-02\n",
      "   -1.68370978e-02  -1.59706556e-04  -2.62603663e-02  -3.68946765e-02\n",
      "    2.69012381e-03   9.98297656e-01]\n",
      " [  2.60661794e-02   2.24900160e-01   9.68807960e-01   3.42563694e-03\n",
      "    3.00250324e-03   9.49375889e-03   2.13269955e-02  -9.78249427e-02\n",
      "    7.56407137e-04   3.94909560e-03]] \n",
      "\n",
      "When n_components= 6 \n",
      "[[ -5.99348611e-04   3.95180460e-03  -9.30313126e-03   9.77013461e-03\n",
      "    7.99743141e-01  -1.15982082e-04   5.94935746e-01   7.26421070e-02\n",
      "    1.73560182e-04   3.14239951e-02]\n",
      " [ -1.57703251e-03  -8.65486079e-03  -1.41729712e-02   1.36618507e-02\n",
      "   -5.96957130e-01   2.48551765e-04   8.01904848e-01  -3.25375266e-03\n",
      "   -1.24914732e-03   1.06619909e-02]\n",
      " [  4.00311919e-03   4.43650130e-02   9.06341269e-02   3.63555020e-03\n",
      "   -6.10090802e-02  -2.42559237e-04  -3.98261666e-02   9.91605188e-01\n",
      "    3.81500215e-04   3.46218809e-02]\n",
      " [ -4.26440126e-04   2.28634173e-02  -1.25300143e-02   1.95012054e-02\n",
      "   -1.68370978e-02  -1.59706556e-04  -2.62603663e-02  -3.68946765e-02\n",
      "    2.69012381e-03   9.98297656e-01]\n",
      " [  2.60661794e-02   2.24900160e-01   9.68807960e-01   3.42563694e-03\n",
      "    3.00250324e-03   9.49375889e-03   2.13269955e-02  -9.78249427e-02\n",
      "    7.56407137e-04   3.94909560e-03]\n",
      " [  3.22021761e-02   9.72157312e-01  -2.28841275e-01   1.09560490e-02\n",
      "   -6.07572126e-03   1.00261862e-02   2.06999697e-03  -2.21244155e-02\n",
      "    8.12056096e-03  -2.62232434e-02]] \n",
      "\n",
      "When n_components= 7 \n",
      "[[ -5.99348611e-04   3.95180460e-03  -9.30313126e-03   9.77013461e-03\n",
      "    7.99743141e-01  -1.15982082e-04   5.94935746e-01   7.26421070e-02\n",
      "    1.73560182e-04   3.14239951e-02]\n",
      " [ -1.57703251e-03  -8.65486079e-03  -1.41729712e-02   1.36618507e-02\n",
      "   -5.96957130e-01   2.48551765e-04   8.01904848e-01  -3.25375266e-03\n",
      "   -1.24914732e-03   1.06619909e-02]\n",
      " [  4.00311919e-03   4.43650130e-02   9.06341269e-02   3.63555020e-03\n",
      "   -6.10090802e-02  -2.42559237e-04  -3.98261666e-02   9.91605188e-01\n",
      "    3.81500215e-04   3.46218809e-02]\n",
      " [ -4.26440126e-04   2.28634173e-02  -1.25300143e-02   1.95012054e-02\n",
      "   -1.68370978e-02  -1.59706556e-04  -2.62603663e-02  -3.68946765e-02\n",
      "    2.69012381e-03   9.98297656e-01]\n",
      " [  2.60661794e-02   2.24900160e-01   9.68807960e-01   3.42563694e-03\n",
      "    3.00250324e-03   9.49375889e-03   2.13269955e-02  -9.78249427e-02\n",
      "    7.56407137e-04   3.94909560e-03]\n",
      " [  3.22021761e-02   9.72157312e-01  -2.28841275e-01   1.09560490e-02\n",
      "   -6.07572126e-03   1.00261862e-02   2.06999697e-03  -2.21244155e-02\n",
      "    8.12056096e-03  -2.62232434e-02]\n",
      " [  7.90352064e-03  -1.23571069e-02  -8.14463189e-04   9.99534596e-01\n",
      "    9.49131814e-04   7.16124510e-03  -1.62050193e-02  -2.96988274e-03\n",
      "    5.65145131e-04  -1.97696119e-02]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5,7+1):\n",
    "    pca = PCA(n_components=i)\n",
    "    pca.fit(X10d)\n",
    "    print \"When n_components=\",i,'\\n',pca.components_,'\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When n_components= 8 \n",
      "[[ -5.99348611e-04   3.95180460e-03  -9.30313126e-03   9.77013461e-03\n",
      "    7.99743141e-01  -1.15982082e-04   5.94935746e-01   7.26421070e-02\n",
      "    1.73560182e-04   3.14239951e-02]\n",
      " [ -1.57703251e-03  -8.65486079e-03  -1.41729712e-02   1.36618507e-02\n",
      "   -5.96957130e-01   2.48551765e-04   8.01904848e-01  -3.25375266e-03\n",
      "   -1.24914732e-03   1.06619909e-02]\n",
      " [  4.00311919e-03   4.43650130e-02   9.06341269e-02   3.63555020e-03\n",
      "   -6.10090802e-02  -2.42559237e-04  -3.98261666e-02   9.91605188e-01\n",
      "    3.81500215e-04   3.46218809e-02]\n",
      " [ -4.26440126e-04   2.28634173e-02  -1.25300143e-02   1.95012054e-02\n",
      "   -1.68370978e-02  -1.59706556e-04  -2.62603663e-02  -3.68946765e-02\n",
      "    2.69012381e-03   9.98297656e-01]\n",
      " [  2.60661794e-02   2.24900160e-01   9.68807960e-01   3.42563694e-03\n",
      "    3.00250324e-03   9.49375889e-03   2.13269955e-02  -9.78249427e-02\n",
      "    7.56407137e-04   3.94909560e-03]\n",
      " [  3.22021761e-02   9.72157312e-01  -2.28841275e-01   1.09560490e-02\n",
      "   -6.07572126e-03   1.00261862e-02   2.06999697e-03  -2.21244155e-02\n",
      "    8.12056096e-03  -2.62232434e-02]\n",
      " [  7.90352064e-03  -1.23571069e-02  -8.14463189e-04   9.99534596e-01\n",
      "    9.49131814e-04   7.16124510e-03  -1.62050193e-02  -2.96988274e-03\n",
      "    5.65145131e-04  -1.97696119e-02]\n",
      " [  9.98261132e-01  -3.77021316e-02  -1.85599207e-02  -8.61671369e-03\n",
      "   -1.02075830e-04   4.01779806e-02   1.26113765e-03  -6.03114313e-04\n",
      "   -4.09164193e-03   1.25188081e-03]] \n",
      "\n",
      "When n_components= 9 \n",
      "[[ -5.99348611e-04   3.95180460e-03  -9.30313126e-03   9.77013461e-03\n",
      "    7.99743141e-01  -1.15982082e-04   5.94935746e-01   7.26421070e-02\n",
      "    1.73560182e-04   3.14239951e-02]\n",
      " [ -1.57703251e-03  -8.65486079e-03  -1.41729712e-02   1.36618507e-02\n",
      "   -5.96957130e-01   2.48551765e-04   8.01904848e-01  -3.25375266e-03\n",
      "   -1.24914732e-03   1.06619909e-02]\n",
      " [  4.00311919e-03   4.43650130e-02   9.06341269e-02   3.63555020e-03\n",
      "   -6.10090802e-02  -2.42559237e-04  -3.98261666e-02   9.91605188e-01\n",
      "    3.81500215e-04   3.46218809e-02]\n",
      " [ -4.26440126e-04   2.28634173e-02  -1.25300143e-02   1.95012054e-02\n",
      "   -1.68370978e-02  -1.59706556e-04  -2.62603663e-02  -3.68946765e-02\n",
      "    2.69012381e-03   9.98297656e-01]\n",
      " [  2.60661794e-02   2.24900160e-01   9.68807960e-01   3.42563694e-03\n",
      "    3.00250324e-03   9.49375889e-03   2.13269955e-02  -9.78249427e-02\n",
      "    7.56407137e-04   3.94909560e-03]\n",
      " [  3.22021761e-02   9.72157312e-01  -2.28841275e-01   1.09560490e-02\n",
      "   -6.07572126e-03   1.00261862e-02   2.06999697e-03  -2.21244155e-02\n",
      "    8.12056096e-03  -2.62232434e-02]\n",
      " [  7.90352064e-03  -1.23571069e-02  -8.14463189e-04   9.99534596e-01\n",
      "    9.49131814e-04   7.16124510e-03  -1.62050193e-02  -2.96988274e-03\n",
      "    5.65145131e-04  -1.97696119e-02]\n",
      " [  9.98261132e-01  -3.77021316e-02  -1.85599207e-02  -8.61671369e-03\n",
      "   -1.02075830e-04   4.01779806e-02   1.26113765e-03  -6.03114313e-04\n",
      "   -4.09164193e-03   1.25188081e-03]\n",
      " [ -4.08021246e-02  -1.02014021e-02  -6.14341646e-03  -6.95244133e-03\n",
      "    2.60043973e-04   9.99036226e-01  -3.10388286e-04   1.44156251e-03\n",
      "   -8.31621523e-03   5.06643363e-04]] \n",
      "\n",
      "When n_components= 10 \n",
      "[[ -5.99348611e-04   3.95180460e-03  -9.30313126e-03   9.77013461e-03\n",
      "    7.99743141e-01  -1.15982082e-04   5.94935746e-01   7.26421070e-02\n",
      "    1.73560182e-04   3.14239951e-02]\n",
      " [ -1.57703251e-03  -8.65486079e-03  -1.41729712e-02   1.36618507e-02\n",
      "   -5.96957130e-01   2.48551765e-04   8.01904848e-01  -3.25375266e-03\n",
      "   -1.24914732e-03   1.06619909e-02]\n",
      " [  4.00311919e-03   4.43650130e-02   9.06341269e-02   3.63555020e-03\n",
      "   -6.10090802e-02  -2.42559237e-04  -3.98261666e-02   9.91605188e-01\n",
      "    3.81500215e-04   3.46218809e-02]\n",
      " [ -4.26440126e-04   2.28634173e-02  -1.25300143e-02   1.95012054e-02\n",
      "   -1.68370978e-02  -1.59706556e-04  -2.62603663e-02  -3.68946765e-02\n",
      "    2.69012381e-03   9.98297656e-01]\n",
      " [  2.60661794e-02   2.24900160e-01   9.68807960e-01   3.42563694e-03\n",
      "    3.00250324e-03   9.49375889e-03   2.13269955e-02  -9.78249427e-02\n",
      "    7.56407137e-04   3.94909560e-03]\n",
      " [  3.22021761e-02   9.72157312e-01  -2.28841275e-01   1.09560490e-02\n",
      "   -6.07572126e-03   1.00261862e-02   2.06999697e-03  -2.21244155e-02\n",
      "    8.12056096e-03  -2.62232434e-02]\n",
      " [  7.90352064e-03  -1.23571069e-02  -8.14463189e-04   9.99534596e-01\n",
      "    9.49131814e-04   7.16124510e-03  -1.62050193e-02  -2.96988274e-03\n",
      "    5.65145131e-04  -1.97696119e-02]\n",
      " [  9.98261132e-01  -3.77021316e-02  -1.85599207e-02  -8.61671369e-03\n",
      "   -1.02075830e-04   4.01779806e-02   1.26113765e-03  -6.03114313e-04\n",
      "   -4.09164193e-03   1.25188081e-03]\n",
      " [ -4.08021246e-02  -1.02014021e-02  -6.14341646e-03  -6.95244133e-03\n",
      "    2.60043973e-04   9.99036226e-01  -3.10388286e-04   1.44156251e-03\n",
      "   -8.31621523e-03   5.06643363e-04]\n",
      " [  3.45755848e-03  -8.38730186e-03   9.82056321e-04  -7.88058439e-04\n",
      "   -7.67708510e-04   8.38147813e-03   9.63150972e-04  -3.08638533e-05\n",
      "    9.99919142e-01  -2.46061844e-03]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(8,10+1):\n",
    "    pca = PCA(n_components=i)\n",
    "    pca.fit(X10d)\n",
    "    print \"When n_components=\",i,'\\n',pca.components_,'\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "* \n",
    "---\n",
    "### Q7:\n",
    "#### Suppose we want to compress the data using just two PCA dimensions. How large is the reconstruction error when doing so\n",
    "* Get the 2-d data and the reconstruction:\n",
    "```\n",
    "X_train_pca = pca.transform(X10d)\n",
    "X_projected = pca.inverse_transform(X_train_pca)\n",
    "```\n",
    "* Calculate the reconstruction error\n",
    "```\n",
    "loss = ((X10d - X_projected) ** 2).mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0475013035966\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X10d)\n",
    "X_train_pca = pca.transform(X10d)\n",
    "X_projected = pca.inverse_transform(X_train_pca)\n",
    "loss = ((X10d - X_projected) ** 2).mean()\n",
    "print loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer Q7:\n",
    "* the reconstruction error is: 0.0475013035966"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "### Q8:\n",
    "#### make a 2-d scatterplot showing the difference between ‘American IPA’ style beers versus all other styles* Get the 2-d data and the reconstruction:\n",
    "* First split the 2-d into X and Y:\n",
    "```\n",
    "x =X_train_pca[:,0]  \n",
    "y =X_train_pca[:,1] \n",
    "```\n",
    "* Then get the color according to its label\n",
    "```\n",
    "cValue = []\n",
    "for i in C:\n",
    "    if i==1:\n",
    "        cValue.append('r')\n",
    "    else:\n",
    "        cValue.append('b')\n",
    "```\n",
    "* Using **matplotlib.pyplot** to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114fda3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x =X_train_pca[:,0]  \n",
    "y =X_train_pca[:,1] \n",
    "C=[0]*(50000/3)\n",
    "for i in range(50000/3):\n",
    "    if data[i]['beer/style']== 'American IPA':\n",
    "        C[i]=1\n",
    "fig = plt.figure()  \n",
    "ax1 = fig.add_subplot(111)  \n",
    "\n",
    "ax1.set_title('Plotting American IPA v.s. others')  \n",
    "plt.xlabel('X')  \n",
    "plt.ylabel('Y')  \n",
    "\n",
    "cValue = []\n",
    "for i in C:\n",
    "    if i==1:\n",
    "        cValue.append('r')\n",
    "    else:\n",
    "        cValue.append('b')\n",
    "\n",
    "ax1.scatter(x,y,c=cValue,marker='.')   \n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
